<!DOCTIPE html>
<html lang="ja">

<<head>
  <meta charset="utf-8" >
  <title>E資格挑戦記　〜課題挑戦の軌跡〜</title>
  <meta name="description" content="E資格受験のための勉強記録を記します。">
  <meta name="keywords" content="E資格">
  <link rel="stylesheet" href="style.css" type="text/css" media ="screen">
</head>


<body>
<!-- #wrapprer ここから-->

<div id="wrapper">
    <header>
      <h1>
        <img src="images/title2.png" alt="E資格挑戦記　〜課題挑戦の軌跡〜">
      </h1>
    </header>

    <div id="left">  </div>

    <div id="right">
      <div class="textarea">

        <h1>深層学習day1</h1>
        <h2>Section1：入力層〜中間層</h2>
        <h2>1.要点</h2>
        <p>入力層の役割は、入力データに対して重みを掛け、バイアスを足し合わすことで,<br>
          中間層に結果を受け渡すこと。<br>
        重みは傾きでバイアスは切片に相当する。<br>
        中間層は入力層から得た値を活性化関数に入力することで出力を得る。<br>
        出力は次のネットワークに受け渡される。<br>
        学習とは、重みとバイアスを最適化することである。<br></p>


        <h2>2.実装演習</h2>
        <a href= "深層学習Day1.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>
        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：ディープラーニングは何をやろうとしているか？何を最適化することが最終目的か？<br>
        A1：目的は重みとバイアス の最適化である。<br><br>

        Q2：次のネットワークを紙にかけ。<br>
        　　入力層：2ノード1層<br>
        　　中間層：3ノード2層<br>
        　　出力層：1ノード1層<br>
        A2：<br>
        <img src="images/2.jpeg" alt="answer" title="netowork", width="600" height="400">
        <br><br>

        Q3：動物分類の実例を入れてみよう<br>
        A3：<br>
        <img src="images/1.jpeg" alt="answer" title="netowork", width="600" height="400">
        <br><br>

        Q4：この数式をpythonで書け<br>
        A4： np.dot(x,W)+b<br><br>

        Q4：中間層の出力を定義している箇所を抜きだせ。<br>
        A4：<br>
        <img src="images/6.png" alt="answer" title="netowork", width="600" height="400">
        <br><br>

        <h2>Section2：活性化関数</h2>
        <h2>1.要点</h2>
        <p>活性化関数は次の入力層への出力の大きさを決める日線型の関数。<br>
          次の層へ強弱をつけてデータを受け渡す。<br>
        活性化関数は中間層用ではReLU/シグモイド/ステップ関数、<br>
        出力層で用いられるものは、シグモイド、ソフトマックス、恒等写像が用いられている。<br>
        スタップ関数は1か0しか出力できない、シグモイドは勾配消失問題がある為、<br>
        ReLU関数が用いられている。<br></p>



        <h2>2.実装演習</h2>
        <a href= "深層学習Day1.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>
        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：線型と非線形の違いについて説明せよ<br>
        A1：線型は1次関数で加法性や斉次性を満たす、非線形は加法性や斉次性を満たさない。<br><br>
        Q2：活性化関数を定義しているコードを抜きだせ<br>
        A2：<br>
        <img src="images/6.png" alt="answer" title="netowork", width="600" height="400">
        <br><br>


        <h2>Section3：出力層</h2>
        <h2>1.要点</h2>
        <p>出力層の役割：中間層が出力したデータを活性化関数に入れて結果を出力すること。<br>
        また、回答と正解との誤差を二乗誤差などで定量化する役割も担う。<br>
        中間層と異なり、信号の大きさの比率が変わらないように、（分類問題では1-0の範囲で限定して）出力する。<br>
        出力層で用いられるものは、シグモイド、ソフトマックス、恒等写像が用いられている。<br>
        回帰では恒等写像、誤差関数は2乗誤差<br>
        2値分類；シグモイド関数、誤差関数は交差エントロピー<br>
        多クラス分類：ソフトマックス関数、誤差関数は交差エントロピーを用いる。<br></p>


        <h2>2.実装演習</h2>
        <a href= "深層学習Day1.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>
        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：2乗する理由、1/2する理由は？<br>
        A1：2乗する理由は、誤差の総和を出したいので正負が混ざると打ち消されてしまう為。<br>
        　　 1/2する理由は、後の誤差逆伝搬で計算が簡単になる為。<br><br>

        Q2：ソフトマックス関数の①〜③の該当の数式に該当するソースコードを示し説明せよ。<br>
        A2：①クラス数と各クラスの出力<br>
        　　②クラスの確率 np.exp(x)<br>
        　　③各クラスの確率の総和　np.sum(np.exp(x))<br><br>

        Q3：①、②に該当するソースコードを示し、一行ずつ処置の説明をせよ。<br>
        A3：yは0と1の出力値、dは出力値。dが最大となる位置で各出力が計算したクラスを計算(np.argmiax()で最大の場所）。<br>
        　　以下の式で交差エントロピーの本計算<br>
        　　-np.sum(np.log(y[np.arange(batch_size),d)] +1e-7))/batch_size<br></p>

        <h2>Section4：勾配降下法</h2>
        <h2>1.要点</h2>
        <p>勾配降下法は、以下の3つの方法が用いられる。<br>
        勾配降下法、確率的勾配降下法、ミニバッチ勾配降下法<br><br>

        ●勾配降下法とは<br>
        勾配降下法は学習率がハイパーパラメータ。<br>
        学習率が大きいと収束しない。<br>
        学習率が小さいと時間がかかる、極小解につかまって脱出できずに学習が進まなくなる。<br>
        勾配降下法の計算方法はAdamなど多数存在する（後で詳細を説明。）<br>
        学習率に従って重み、バイアスを更新する回数の単位をエポックという。<br><br>

        ●確率的勾配降下法とは<br>
        ランダムに抽出したサンプルで学習することで、極小解の回避とオンライン学習をできる事<br>
        データのメモリの節約が可能。<br><br>

        ●ミニバッチ勾配降下法<br>
        バッチを分割してメモリに乗り切らないデータを分割する。<br>
        メモリの効率的な利用、スレッド並列化、GPUを用いたSIMD並列化が可能。<br>
        SIMD(Single Instruction Muiti Data)<br></p>



        <h2>2.実装演習</h2>
        <a href= "深層学習Day1.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>

        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：勾配降下法に該当するソースコードを抜き出してみましょう<br>
        A1：<br>
        <img src="images/7.png" alt="answer" title="netowork", width="600" height="200">
        <br><br>
        Q2：オンライン学習とは何か<br>
        A2：データを追加してパラメータ（重みとバイアス）を学習できる。<br>
        　　バッチ学習は一度にデータをすべて投入する<br><br>

        Q3：パラメータ更新の式の意味を図で説明せよ。<br>
        A3：<br>
        <img src="images/3.jpeg" alt="answer" title="netowork", width="600" height="400">
        <br><br>

        <h2>Section5：誤差逆伝播法</h2>
        <h2>1.要点</h2>
        <p>勾配計算には微分を用いる。<br>
        数値微分(微小な差分加えて足し引きすることで傾きを求める)では計算量が大きい為使用が現実的ではない。<br>
        →誤差逆伝播を用いる<br><br>

        ●誤差逆伝播とは<br>
        逆に微分していくことで微分の連鎖律により計算を簡略化できる。<br>
        一度計算した結果を再利用できるため計算が効率化される。<br>
        自分最適化したい重み、バイアスまで微分して辿っていくことで計算できる。<br><br></p>

        <h2>2.実装演習</h2>
        <a href= "深層学習Day1.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>

        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：誤差逆伝播法では不要な再起的処理を避けることができる。<br>
           すでに行なった計算結果を保持しているソースコードを抽出せよ。<br>
        A1：<br>
        <img src="images/8.png" alt="answer" title="netowork", width="600" height="600">
        <br><br>
        Q2：空欄に該当するソースコードを探せ。<br>
        A2：<br>
        <img src="images/5.jpeg" alt="answer" title="netowork", width="600" height="400">
        <br><br>
        Q3：連鎖率をの原理を使いdz/dxを求めよ。<br>
            z=t^2,t=x+y<br>
        A4：<br>
        <img src="images/4.jpeg" alt="answer" title="netowork", width="600" height="400">
        <br><br>
        <h1>深層学習day2</h1>
        <h2>Section1：勾配消失問題</h2>
        <h2>1.要点</h2>
        <p>勾配消失とは、多層のネットワークを形成することで微分ちが0-1の範囲の値のため、<br>
          逆伝搬で値が0に近づいていってしまう現象。<br>
        特に、シグモイド関数では数値が大きくなるほど、傾きが0に近づくために勾配消失を引き起こしやすい。<br><br>

        解決方法<br>
        1)活性化関数の選択<br>
        ReLU関数を使用する。勾配消失の回避とスパース化に貢献することで勾配消失を抑制。<br>
        現在の主流の活性化関数<br><br>

        2)重みの初期設定<br>
        一般的に正規分布に従う乱数を使って重みを与える。<br>
        Xavier初期化：正規分布を前のレイヤーのノード数のルートで割ることで、偏りのない出力が得られる。<br>
        　　　　　主にs字カーブの曲線に対して適用可能。<br>
        ReLU関数などのs字カーブでない活性化関数に対しては、Heの初期値を用いる。<br>
        He初期値＝標準偏差/(2/前のノードのレイヤー数）のルート<br><br>

        3)バッチ正規化<br>
        ミニバッチ学習でバッチごとのデータの入力の偏りを抑制する手法。<br>
        バッチ正規化を活性化関数の前に入れて入力の偏りをなくす。<br></p>

        <h2>2.実装演習</h2>
        <a href= "深層学習Day2.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>

        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：シグモイド関数を微分したとき、入力が0の時に最大値を取る。その値はいくつか<br>
        A1：0.25<br><br>

        Q2：重みの初期値が0設定するとどのような問題が発生するか<br>
        A2：正しく学習できない均一に更新されるために複雑な学習ができなくなる<br><br>

        Q3：一般的に考えられるバッチ正規化の効果を２点挙げよ。<br>
        A3：バッチ間のデータの偏りをなくす。補正をするため、勾配消失が起きにくくなる。<br><br></p>

        <h2>Section2：学習率最適化法</h2>
        <h2>1.要点</h2>
        <p>学習率のを最初は大きく、後半小さくしていくことが望ましい。<br>
        1）モメンタム<br>
        　前回の学習の重みを足し合わせることで、①局所最適解の回避（大域的最適解への到達）②谷間にいても早く最適化できる<br>
        　勾配降下法：ジグザグ運転に対して、モメンタムは移動平均のような移動<br><br>

        2）AdaGrad<br>
        　学習率を前の誤差を加味して再設定する方法。誤差が小さい領域で最適解に近づきやすい。<br>
        　緩やかな斜面になったら学習率が小さくなるため、鞍点問題を引き起こしやすい<br><br>

        3）RMSProp<br>
         AdaGradの発展系、前回の誤差を反映する度合いをαで表現することで、<br>
         鞍点問題を解決し、大域的最適解を設定できる。<br><br>

        4）Adam<br>
        　モメンタム、RMPPropのメリットを取り込んだアルゴリズム。<br>
        　最もよく用いられている。<br><br></p>

        <h2>2.実装演習</h2>
        <a href= "深層学習Day2.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>
        　

        <h2>Section3：過学習</h2>
        <h2>1.要点</h2>
        <p>過学習とは、トレーニングデータに適合しすぎてテストデータに対して正答率が上がらなくなる現象。<br>
        過学習の原因は重みが極端に大きくなることで情報を課題評価することで発生する。<br>
        そのため、極端な重みの増加を防ぐことが基本の対策となる。<br>
        自由度が高すぎることで発生するため、自由度を下げることが手段として用いられる。<br><br>

        主に以下の正則化を手法として用いられる<br>
        1）L1/L2正則化<br>
        誤差関数にpノルムを加える。p=1でＬ１正則化（Lasso）、p=2でL2正則化(Rigge)という。<br>
        ・p1ノルムは、距離成分の積算、道のり距離ーマンハッタン距離<br>
        ・p2ノルムは、距離の絶対値（x^2+y^2）^0.5ーユーグリッド距離<br>
        P1ノルムでのスパース化により無駄な情報がなくなり、勾配降下がスムーズになる。<br><br>


        2）ドロップアウト<br>
        ランダムにノードを削除して学習される。データ量を変化させずに学習を進めさせる。<br><br></p>


        <h2>2.実装演習</h2>
        <a href= "深層学習Day2.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>

        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：L1正則化について表しているグラフはどちらか。<br>
        A1：右図（Lasso）。理由はマンハッタン距離を用いることで、<br>
        軸上以外の自由度をなくす性質を持つため、<br>
        等高線が菱形になる性質を持つためである。<br><br></p>

        <h2>Section4：畳み込みニューラルネットワークの概念</h2>
        <h2>1.要点</h2>
        <p>CNNでは次元間でつながりのあるデータを扱う。（画像やスペクトルなど）<br>
        1）畳み込み、2）プーリングを行い、出力を行う。<br>
        1）畳み込み：特徴量を抽出するため、フィルター画像を端から順に掛け合わせた結果を出力し、畳み込み画像を得ること。<br>
        　畳み込み演算のためには、①バイアス、②パディング 、③ストライドの3つの概念が用いられている。<br>
        　①バイアス：畳み込み後のデータに足す数字。<br>
        　②パディング：畳み込みで出力画像が小さくなるのを防ぐために画像の外側を補間すること。0や最近傍値で埋める方法が一般的。<br>
        　③ストライド：フィルターの移動幅。<br>
        2）プーリングは重みを使わずにフィルタ内の最大値、平均値などの特性値を出力する。<br></p>

        <h2>2.実装演習</h2>
        <a href= "深層学習Day2.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>

        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>Q1：サイズ6ｘ6の入力画像をサイズ2x2のフィルタで畳み込んだ時の出力画像のサイズを答えよ。<br>
        　　　ストライドとパディングは１とする。<br>
        A1：7x7<br><br>

        Q2：サイズ5ｘ5の入力画像をサイズ2x2のフィルタで畳み込んだ時の出力画像のサイズを答えよ。<br>
        　　　ストライドは2、パディングは１とする。<br>
        A2：3x3<br></p>

        <h2>Section5：最新のCNN</h2>
        <h2>1.要点</h2>
        <p>AlexNetのモデルの説明<br>
        224x224の画像を使用。<br>
        Flatten処理で画像を数字の配列に変換する処理を用いている。<br>
        GlobalMaxPoolingやGlobalAveragePoolingで効率的にデータの特徴量を抽出できる。<br>
        全結合層にドロップアウトを適用。一般的にドロップアウトは全結合層に適用される。<br><br></p>
        <h2>2.実装演習</h2>
        <a href= "深層学習Day2.html" >＊＊Section1〜5までまとめて掲載＊＊</a><br>
        <h2>3.確認テスト考察/追加レポートなど</h2>
        <p>●追加レポート<br>
        最新のCNNモデルであるSEnet(Squeeze-and-Excitation Networks)を学習。<br>
        SEBlockと呼ばれる以下の工程を経ることが特徴である。<br>
        ①inputデータに対してGlobalAveragePoolingを適用することで計算を簡略化<br>
        ②Encode/Decode（ReLu関数によるデータ範囲の値の簡素化とシグモイド関数によるデータ範囲の狭小化）<br>
        ①/②で得た値をInputデータに掛け合わせることで重み付をする。<br>
        これにより、計算負荷をほぼ上げることなく、性能向上を実現できる<br></p>

      </div>

    </div>

    <div id="clearfix"></div>
</div>
<!-- #wrapprer ここまで-->

<footer></footer>
</body>
